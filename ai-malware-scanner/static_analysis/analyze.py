import os
import logging
import numpy as np
import lief
import hashlib
import magic
import re
from collections import Counter
from entropy import shannon_entropy

# Initialize logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_imphash(binary):
    """Calculate import hash for PE file using LIEF."""
    try:
        imports = []
        for imported_library in binary.imports:
            library_name = imported_library.name.lower()
            for function in imported_library.entries:
                if function.name:
                    imports.append(f"{library_name}.{function.name}")
        imports = sorted(imports)
        import_str = ",".join(imports)
        return hashlib.md5(import_str.encode()).hexdigest()
    except Exception as e:
        logger.error(f"Error calculating imphash: {e}")
        return None

def extract_static_features(file_path):
    """Extract comprehensive static features from binary files using LIEF."""
    try:
        # Basic file info
        file_size = os.path.getsize(file_path)
        file_magic = magic.from_file(file_path)
        file_type = magic.from_file(file_path, mime=True)
        
        # Calculate hashes
        with open(file_path, "rb") as f:
            file_data = f.read()
            md5_hash = hashlib.md5(file_data).hexdigest()
            sha1_hash = hashlib.sha1(file_data).hexdigest()
            sha256_hash = hashlib.sha256(file_data).hexdigest()
        
        # Check if file is a PE file
        is_pe = False
        features = {
            "file_size": file_size,
            "file_type": file_type,
            "file_magic": file_magic,
            "md5": md5_hash,
            "sha1": sha1_hash,
            "sha256": sha256_hash,
            "entropy": shannon_entropy(file_data)
        }
        
        # PE specific features
        if "PE32" in file_magic or "PE32+" in file_magic:
            is_pe = True
            binary = lief.parse(file_path)
            if binary:
                # Header info
                features.update({
                    "is_pe": True,
                    "imphash": get_imphash(binary),
                    "entry_point": binary.optional_header.addressof_entrypoint,
                    "image_base": binary.optional_header.imagebase,
                    "num_sections": len(binary.sections),
                    "num_directories": len(binary.data_directories),
                    "compile_time": binary.header.time_date_stamps,
                    "virtual_size": binary.optional_header.sizeof_image,
                    "has_debug": binary.has_debug,
                    "has_tls": binary.has_tls,
                    "has_resources": binary.has_resources,
                    "has_relocations": binary.has_relocations,
                    "has_signature": binary.has_signatures,
                    "has_nx": binary.optional_header.has_nx,
                    "has_no_seh": binary.optional_header.has_no_seh,
                })
                
                # Import features
                imports = Counter()
                if binary.has_imports:
                    for imported_library in binary.imports:
                        for function in imported_library.entries:
                            if function.name:
                                imports[function.name] += 1
                features["num_imports"] = sum(imports.values())
                
                # Section features
                section_names = [section.name for section in binary.sections]
                section_entropies = [shannon_entropy(section.content) for section in binary.sections]
                section_sizes = [section.size for section in binary.sections]
                features["sections"] = section_names
                features["section_entropies"] = section_entropies
                features["section_sizes"] = section_sizes
                features["avg_section_entropy"] = np.mean(section_entropies) if section_entropies else 0
        else:
            features["is_pe"] = False
        
        # Extract strings for all file types
        strings_pattern = re.compile(b'[\x20-\x7f]{5,}')
        strings = strings_pattern.findall(file_data)
        features["strings"] = [s.decode(errors='ignore') for s in strings[:1000]]  # Limit to first 1000 strings
        features["num_strings"] = len(strings)
        
        return features
    except Exception as e:
        logger.error(f"Error processing file {file_path}: {e}")
        return None

def vectorize_features(features):
    """Convert extracted features to numerical vector for model input."""
    if not features or not features.get("is_pe", False):
        return None
    
    # Create feature vector (expand as needed based on model requirements)
    feature_vector = [
        features.get("file_size", 0),
        features.get("entropy", 0),
        features.get("entry_point", 0),
        features.get("image_base", 0),
        features.get("num_sections", 0),
        features.get("num_imports", 0),
        features.get("num_strings", 0),
        features.get("avg_section_entropy", 0),
        features.get("virtual_size", 0),
        int(features.get("has_debug", False)),
        int(features.get("has_tls", False)),
        int(features.get("has_resources", False)),
        int(features.get("has_relocations", False)),
        int(features.get("has_signature", False)),
        int(features.get("has_nx", False)),
        int(features.get("has_no_seh", False)),
    ]
    
    return feature_vector
