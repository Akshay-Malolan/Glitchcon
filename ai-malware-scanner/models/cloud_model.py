import os
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F
import smtplib
from email.mime.text import MIMEText
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleNN(nn.Module):
    """
    A simple neural network model for binary classification.
    Fallback model when loading from file fails.
    """
    def __init__(self, input_size=2381, hidden_sizes=[64, 32, 16]):
        super(SimpleNN, self).__init__()
        
        # Input layer
        layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]
        
        # Hidden layers
        for i in range(len(hidden_sizes)-1):
            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))
            layers.append(nn.ReLU())
            
        # Output layer (binary classification)
        layers.append(nn.Linear(hidden_sizes[-1], 2))
        layers.append(nn.Sigmoid())
        
        self.model = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.model(x)

class ModelWrapper:
    _instance = None
    
    @classmethod
    def get_instance(cls, *args, **kwargs):
        """Singleton pattern to prevent multiple model loads"""
        if cls._instance is None:
            cls._instance = cls(*args, **kwargs)
        return cls._instance
    
    def __init__(self, model_path="models/classic_mlp.pt", model_type="pytorch"):
        """
        Initialize the model wrapper
        
        Args:
            model_path: Path to the model file
            model_type: Type of model (pytorch, svm, qsvm)
        """
        self.model_type = model_type
        # Ensure the models directory exists
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        # Available models in the repository
        self.available_models = {
            "classic_mlp": {"url": "https://github.com/fabiocaiulo8/malware-detection/raw/main/models/classic_mlp.pt", "type": "pytorch"},
            "hybrid_mlp": {"url": "https://github.com/fabiocaiulo8/malware-detection/raw/main/models/hybrid_mlp.pt", "type": "pytorch"},
            "bagging_svm": {"url": "https://github.com/fabiocaiulo8/malware-detection/raw/main/models/bagging_svm.pkl", "type": "svm"},
            "linear_svm": {"url": "https://github.com/fabiocaiulo8/malware-detection/raw/main/models/linear_svm.pkl", "type": "svm"},
            "bagging_qsvm": {"url": "https://github.com/fabiocaiulo8/malware-detection/raw/main/models/bagging_qsvm.pkl.xz", "type": "qsvm"},
            "linear_qsvm": {"url": "https://github.com/fabiocaiulo8/malware-detection/raw/main/models/linear_qsvm.pkl", "type": "qsvm"}
        }
        
        # Download model if it doesn't exist
        if not os.path.exists(model_path):
            logger.info(f"Downloading pre-trained model from GitHub...")
            model_name = os.path.basename(model_path)
            model_key = model_name.split('.')[0]  # Get model name without extension
            
            if model_key in self.available_models:
                model_url = self.available_models[model_key]["url"]
                self.model_type = self.available_models[model_key]["type"]
                self._download_file(model_url, model_path)
            else:
                # Default to classic_mlp if model name not recognized
                logger.info(f"Model {model_name} not recognized. Creating a fallback model.")
                self.model_type = "pytorch"
                self.model_path = model_path
        
        # Create a default scaler since scaler.pkl isn't available in the repository
        self.scaler = StandardScaler()
        logger.info("Using a default StandardScaler. For optimal results, the scaler should be fitted on training data.")
        
        # Set default input size - this is based on the EMBER dataset column count
        self.input_size = 2381  
        
        # Load or create the model
        try:
            if self.model_type == "pytorch":
                try:
                    # Try to load the PyTorch model
                    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
                    self.model = torch.load(model_path, map_location=device)
                    logger.info(f"PyTorch model loaded successfully from {model_path}")
                    
                    # Ensure model is in evaluation mode
                    self.model.eval()
                    
                    # Try to determine input size from model
                    if hasattr(self.model, 'model') and hasattr(self.model.model[0], 'in_features'):
                        self.input_size = self.model.model[0].in_features
                        logger.info(f"Model input size determined: {self.input_size}")
                    
                except Exception as e:
                    logger.error(f"Error loading PyTorch model: {e}")
                    logger.info("Creating a fallback model...")
                    self.model = self._create_fallback_model()
                    
                    # Save fallback model
                    torch.save(self.model, model_path)
                    logger.info(f"Fallback model saved to {model_path}")
                
                # Verify model works by testing with dummy input
                self._test_model()
                
            elif self.model_type in ["svm", "qsvm"]:
                self.model = joblib.load(model_path)
                logger.info(f"SVM model loaded from {model_path}")
                
                # Default input size for SVMs, should be updated based on specific model
                self.input_size = 16
                logger.info(f"Using default input size for SVM: {self.input_size}")
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            logger.info("Creating a fallback model...")
            self.model = self._create_fallback_model()
    
    def _create_fallback_model(self):
        """Create a simple PyTorch model when loading fails"""
        logger.info(f"Creating a simple fallback neural network with input size {self.input_size}")
        
        # Create a simple neural network
        model = SimpleNN(input_size=self.input_size)
        
        # Set to evaluation mode
        model.eval()
        
        logger.info("Fallback model created successfully")
        return model
    
    def _download_file(self, url, path):
        """Download a file from a URL to the specified path"""
        try:
            response = requests.get(url)
            if response.status_code == 200:
                with open(path, 'wb') as f:
                    f.write(response.content)
                logger.info(f"File downloaded to {path}")
                return True
            else:
                logger.error(f"Failed to download file: HTTP {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"Error downloading file: {e}")
            return False
    
    def _test_model(self):
        """Test the model with dummy input to ensure it works"""
        try:
            # Create a dummy input tensor
            dummy_input = torch.zeros((1, self.input_size), dtype=torch.float32)
            
            # Run inference
            with torch.no_grad():
                result = self.model(dummy_input)
                
            logger.info("Model verification successful!")
        except Exception as e:
            logger.error(f"Model verification failed: {e}")
            logger.info("Recreating model after verification failure...")
            self.model = self._create_fallback_model()
            
            # Test again
            dummy_input = torch.zeros((1, self.input_size), dtype=torch.float32)
            with torch.no_grad():
                result = self.model(dummy_input)
            logger.info("Fallback model verification successful!")
    
    def _send_alert(self, prediction):
        """
        Send an automated alert for high-risk predictions.
        
        Args:
            prediction: Dictionary containing prediction results.
        """
        try:
            if prediction.get("is_malicious") and prediction.get("confidence", 0) > 0.9:
                # Configure email settings
                sender_email = "alert@example.com"
                recipient_email = "admin@example.com"
                subject = "High-Risk Malware Detected"
                body = f"Malware detected with confidence {prediction['confidence']:.2f}.\nDetails: {prediction}"

                # Create email message
                msg = MIMEText(body)
                msg["Subject"] = subject
                msg["From"] = sender_email
                msg["To"] = recipient_email

                # Send email
                with smtplib.SMTP("smtp.example.com", 587) as server:
                    server.starttls()
                    server.login("alert@example.com", "password")
                    server.sendmail(sender_email, recipient_email, msg.as_string())
                logger.info("Alert sent successfully.")
        except Exception as e:
            logger.error(f"Failed to send alert: {e}")

    def predict(self, features):
        """
        Predict if a sample is malicious based on its features
        
        Args:
            features: List of numerical features
            
        Returns:
            Dictionary with prediction results
        """
        try:
            if self.model is None:
                logger.error("Model not loaded properly")
                return {"error": "Model not loaded properly. Cannot make predictions."}
            
            # Convert to numpy array if it's not already
            if not isinstance(features, np.ndarray):
                features = np.array(features, dtype=np.float32)
            
            # Reshape if it's a single sample
            if len(features.shape) == 1:
                features = features.reshape(1, -1)
                
            # Adjust feature dimensions if needed
            if features.shape[1] != self.input_size:
                logger.warning(f"Feature size mismatch. Expected {self.input_size}, got {features.shape[1]}. Attempting to adjust...")
                
                if features.shape[1] < self.input_size:
                    # Pad with zeros
                    padded_features = np.zeros((features.shape[0], self.input_size), dtype=np.float32)
                    padded_features[:, :features.shape[1]] = features
                    features = padded_features
                    logger.info(f"Features padded to expected size {self.input_size}")
                else:
                    # Truncate
                    features = features[:, :self.input_size]
                    logger.info(f"Features truncated to expected size {self.input_size}")
            
            if self.model_type == "pytorch":
                # For PyTorch models
                # Scale the features
                scaled_features = self.scaler.fit_transform(features)
                
                # Convert to PyTorch tensor
                tensor_features = torch.tensor(scaled_features, dtype=torch.float32)
                
                # Set model to evaluation mode and disable gradient calculation
                self.model.eval()
                with torch.no_grad():
                    # Get prediction
                    output = self.model(tensor_features)
                    
                    # Convert output to probabilities
                    if isinstance(output, tuple):
                        output = output[0]  # Some models return (output, hidden_states)
                    
                    # Apply softmax if output doesn't sum to 1
                    output_sum = torch.sum(output).item()
                    if abs(output_sum - 1.0) > 0.1:  # Not already normalized
                        probabilities = F.softmax(output, dim=1)
                    else:
                        probabilities = output
                    
                    # Convert to numpy for further processing
                    probabilities = probabilities.cpu().numpy()
                    
                    # Get predicted class and confidence
                    predicted_class = np.argmax(probabilities, axis=1)[0]
                    confidence = float(probabilities[0][predicted_class])
                    
                    result = {
                        "is_malicious": bool(predicted_class),
                        "confidence": confidence,
                        "label": "malicious" if predicted_class == 1 else "benign",
                        "raw_probabilities": [float(p) for p in probabilities[0]]
                    }
                    
                    # Send alert if necessary
                    self._send_alert(result)
                    return result
            
            elif self.model_type in ["svm", "qsvm"]:
                # For SVM models, we'll skip scaling since most SVM implementations have built-in scaling
                try:
                    # Try to use predict_proba if available (for probability estimates)
                    probabilities = self.model.predict_proba(features)
                    predicted_class = np.argmax(probabilities, axis=1)[0]
                    confidence = float(probabilities[0][predicted_class])
                    
                    result = {
                        "is_malicious": bool(predicted_class),
                        "confidence": confidence,
                        "label": "malicious" if predicted_class == 1 else "benign",
                        "raw_probabilities": [float(p) for p in probabilities[0]]
                    }
                    
                    # Send alert if necessary
                    self._send_alert(result)
                    return result
                except:
                    # Fall back to binary prediction
                    prediction = self.model.predict(features)[0]
                    result = {
                        "is_malicious": bool(prediction),
                        "confidence": 1.0,  # No probability available
                        "label": "malicious" if prediction else "benign",
                        "raw_probabilities": [1.0 if prediction else 0.0, 0.0 if prediction else 1.0]
                    }
                    
                    # Send alert if necessary
                    self._send_alert(result)
                    return result
                
        except Exception as e:
            logger.error(f"Error during prediction: {e}")
            return {"error": str(e)}
    
    def cloud_scan(self, features):
        """
        Placeholder for cloud-based scanning.
        
        Args:
            features: List of numerical features.
        
        Returns:
            Dictionary with prediction results.
        """
        logger.info("Cloud-based scanning is not yet implemented.")
        return {"error": "Cloud-based scanning not implemented."}

# Create a singleton instance of the model that can be imported by other modules
model = ModelWrapper.get_instance()
