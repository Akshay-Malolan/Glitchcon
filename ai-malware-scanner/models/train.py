import os
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
import requests

# PyDrive for Google Drive upload
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

# ---------------------------
# Configuration Parameters
# ---------------------------
# Public dataset URL (e.g., a GitHub raw link to ember.csv)
DATASET_URL = "https://raw.githubusercontent.com/yourusername/yourrepo/main/ember.csv"  # Update with your actual URL
DATASET_PATH = "ember.csv"

# Feature columns and label (update based on your dataset)
FEATURE_COLUMNS = ["feature1", "feature2", "feature3", "feature4"]  # Replace with actual feature names
LABEL_COLUMN = "label"  # 1 for malware, 0 for benign

# Local paths for model persistence
LOCAL_MODEL_PATH = "models/malware_model.pth"
INPUT_SIZE_PATH = "models/input_size.pkl"

# ---------------------------
# Download the Dataset if Not Present
# ---------------------------
if not os.path.exists(DATASET_PATH):
    print("Downloading dataset from public URL...")
    response = requests.get(DATASET_URL)
    if response.status_code == 200:
        with open(DATASET_PATH, "wb") as f:
            f.write(response.content)
        print("Dataset downloaded successfully.")
    else:
        raise Exception("Failed to download dataset. Status code: " + str(response.status_code))

# ---------------------------
# Load and Prepare the Dataset
# ---------------------------
data = pd.read_csv(DATASET_PATH)
X = data[FEATURE_COLUMNS].values
y = data[LABEL_COLUMN].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ---------------------------
# Define the PyTorch Model
# ---------------------------
class MalwareClassifier(nn.Module):
    def __init__(self, input_size):
        super(MalwareClassifier, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 2)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return torch.softmax(x, dim=1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_size = X_train.shape[1]
model = MalwareClassifier(input_size).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ---------------------------
# Train the Model
# ---------------------------
X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)

for epoch in range(10):  # Adjust epochs as needed
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Ensure models directory exists
os.makedirs("models", exist_ok=True)
torch.save(model.state_dict(), LOCAL_MODEL_PATH)
joblib.dump(input_size, INPUT_SIZE_PATH)
print("Model training complete and saved locally.")

# ---------------------------
# Upload the Model to Google Drive using PyDrive
# ---------------------------
print("Uploading model to Google Drive...")

gauth = GoogleAuth()
gauth.LocalWebserverAuth()  # This will prompt you to authenticate via a browser
drive = GoogleDrive(gauth)

gfile = drive.CreateFile({'title': os.path.basename(LOCAL_MODEL_PATH)})
gfile.SetContentFile(LOCAL_MODEL_PATH)
gfile.Upload()
print("Model uploaded to Google Drive successfully!")
print("Google Drive File ID:", gfile['id'])
